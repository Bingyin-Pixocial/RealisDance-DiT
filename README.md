# RealisDance-DiT: Simple yet Strong Baseline towards Controllable Character Animation in the Wild

<a href='https://arxiv.org/abs/2504.14977'>
<img src='https://img.shields.io/badge/arXiv-red'></a> 
<a href='https://thefoxofsky.github.io/project_pages/RealisDance-DiT/index'>
<img src='https://img.shields.io/badge/Project-Page-orange'></a> 
<a href='https://thefoxofsky.github.io/project_pages/RealisDance-DiT/index'>
<img src='https://img.shields.io/badge/Test-Dataset-blue'></a> 


This repository is the official implementation of [RealisDance-DiT](https://arxiv.org/abs/2504.14977).
RealisDance-DiT is a structurally simple, empirically robust, and experimentally strong baseline model for controllable character animation in the wild.

## News
- **2025-05-19**: Released RealisDance-DiT inference code and weights.
- **2024-05-19**: You may also be interested in our [Uni3C](https://github.com/ewrfcas/Uni3C).
- **2024-10-15**: Released the code of pose preparation for RealisDance.
- **2024-09-10**: Now you can try more interesting AI video editing in [XunGuang](https://xunguang.damo-vision.com/).
- **2024-09-09**: You may also be interested in our human part repair method [RealisHuman](https://github.com/Wangbenzhi/RealisHuman).

## Gallery
Here are several character animation generated by RealisDance-DiT. 
Note that the GIFs shown here have some degree of visual quality degradation.
Please visit our [project page](https://thefoxofsky.github.io/project_pages/RealisDance-DiT/index) for more original videos 

<table class="center">
    <tr>
    <td><img src="__assets__/samples/1.gif"></td>
    <td><img src="__assets__/samples/2.gif"></td>
    </tr>
    <tr>
    <td><img src="__assets__/samples/3.gif"></td>
    <td><img src="__assets__/samples/4.gif"></td>
    </tr>
    <tr>
    <td><img src="__assets__/samples/5.gif"></td>
    <td><img src="__assets__/samples/6.gif"></td>
    </tr>
</table>

## TODO List
- [ ] SMPL retargeting 
- [ ] TeaCache speedup
- [ ] FSDP + Sequential parallel
- [ ] Pose paration code
- [ ] RealisDance-Val dataset
- [x] Model checkpoints
- [x] Inference code

Note: This released project has two slight differences between the paper.

1. We only use SMPL-CS and HaMer in this version in order to support Uni3C. 
Because it is difficult to align and render DWPose in new camera view.
2. (Coming soon) In the Shifted RoPE part, we also shifted frame. 
Because we find that share the first frame RoPE will sometimes introduce 
slight artifacts at the first frame.


## Quick Start

### 1. Setup Repository and Environment

```
git clone https://github.com/theFoxofSky/RealisDance.git
cd RealisDance

conda create -n realisdance python=3.10
conda activate realisdance

pip install -r requirements.txt

# FA3 (Optional)
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention
git checkout ea3ecea97a1393c092863330aff9a162bb5ce443  # very important, using other FA3 will yield bad results
cd hopper
python setup.py install
```

### 2. Quick Inference

- Inference with Demo sequences

```commandline
python inference.py \
    --ref __assets__/demo/ref.png \
    --smpl __assets__/demo/smpl.mp4 \
    --hamer __assets__/demo/hamer.mp4 \
    --prompt "A blonde girl is doing somersaults on the grass. Behind the grass is a river, \
    and behind the river are trees and mountains. The girl is wearing black yoga pants and a black sports vest." \
    --save-dir ./output
```

### 3. Custom Batch Inference

- Prepare your reference images and conditions (coming soon)

```commandline
TODO
```

- The root dir should be structured as:
```
root/
|---ref/
    |---1.png
    |---2.png
    |---...
|---smpl/
    |---1.mp4
    |---2.mp4
    |---...
|---hamer/
    |---1.mp4
    |---2.mp4
    |---...
|---prompt/
    |---1.txt
    |---2.txt
    |---...
```

- Batch inference

```commandline
python inference.py --save-dir ./output --root $PATH-TO-ROOT-DIR
```

## Disclaimer
This project is released for academic use.
We disclaim responsibility for user-generated content.

## Contact Us
Jingkai Zhou: [fs.jingkaizhou@gmail.com](mailto:fs.jingkaizhou@gmail.com)


## BibTeX
```
@article{zhou2025realisdance-dit,
  title={RealisDance: Equip controllable character animation with realistic hands},
  author={Zhou, Jingkai and Wu, Yifan and Li, Shikai and Wei, Min and Fan, Chao and Chen, Weihua and Jiang, Wei and Wang, Fan},
  journal={arXiv preprint arXiv:2504.14977},
  year={2025}
}

@article{zhou2024realisdance,
  title={RealisDance: Equip controllable character animation with realistic hands},
  author={Zhou, Jingkai and Wang, Benzhi and Chen, Weihua and Bai, Jingqi and Li, Dongyang and Zhang, Aixi and Xu, Hao and Yang, Mingyang and Wang, Fan},
  journal={arXiv preprint arXiv:2409.06202},
  year={2024}
}
```


## Acknowledgements
Thanks [Shikai Li](https://scholar.google.com/citations?user=WXGg2rgAAAAJ&hl) for condition paraperation and [Chenjie Cao](https://ewrfcas.github.io/) for pose align.